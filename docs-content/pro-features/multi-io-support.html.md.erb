---
title: Multi-IO stream support
owner: Spring Cloud Data Flow Release Engineering
---

Spring Cloud Stream includes built-in support for binding multi-IO applications to the messaging system, and <%= vars.scdf_product_full_name %> (<%= vars.scdf_product_short_name %>) can embed multi-IO applications into a streaming pipeline.
Here are some highlights of the multi-IO support in <%= vars.product_short_name %> and answers some frequently-asked questions.

Spring Cloud Data Flow OSS currently only supports streaming pipelines built using applications that have a single input and output.
The SCDF Pro server adds support for creating and deploying streaming pipelines using applications that have multiple inputs and outputs (this is referred to as "multi-IO").

Here are some highlights of the multi-IO support in <%= vars.product_short_name %> and answers some frequently-asked questions.

## <a id='multi-io-support-Q1'></a> What are the multi-IO features in the SCDF Pro Server?

SCDF Pro adds a number of features to support multi-IO stream applications:

* A multi-IO stream DSL for constructing streaming pipelines using applications that have multiple inputs and outputs
* A Stream Flo editor for wiring up multi-IO applications when creating a stream
* Lifecycle management for multi-IO streams

## <a id='multi-io-support-Q2'></a> How do multi-IO applications differ From single-IO applications?

SCDF previously included support for registering a stream application that had a single input and output, where the application input and output endpoints had to be configured as the `input` and `output` endpoints using the Spring Cloud Stream `@EnableBinding` annotation.
With the new functional support added to Spring Cloud Stream, as well as configurable function bindings, you can choose named endpoints for the inbound and outbound bindings of your function.

For instance, consider a function that has multiple inputs and a single output:

```
@Bean
public BiFunction<String, Long, String> process() {

  return (inputString, inputLong) -> {
	// Process inputs and produce output
  };
}
```

In this example, you could define the following function bindings:

```
spring.cloud.function.defintion: process
spring.cloud.stream.function.bindings.process-in-0: myInput1
spring.cloud.stream.function.bindings.process-in-1: myInput2
spring.cloud.stream.function.bindings.process-out-0: processedOutput
```

After defining function bindings as logical names, you can configure the logical names as inbound and outbound ports for your application.
Create <%= vars.scdf_image %> labels with the following key / value pairs:

```
configuration-properties.outbound-ports/processedOutput
configuration-properties.inbound-ports/myInput1,myInput2
```

<% if vars.scdf_product == 'K8s' %>
You can use any build tool to apply your label to the container image.
This example assumes that you used the Maven Jib plugin to generate the <%= vars.scdf_image %>.
Add the inbound and outbound ports as <%= vars.scdf_image %> labels, as shown in the following example:

```
<container>
  <creationTime>USE_CURRENT_TIMESTAMP</creationTime>
  <format>Docker</format>
  <labels>
    <configuration-properties.outbound-ports>processedOutput</configuration-properties.outbound-ports>
    <configuration-properties.inbound-ports>myInput1,myInput2</configuration-properties.inbound-ports>
  </labels>
</container>
```
<% end %>

<% if vars.scdf_product == 'TAS' %>
You can use the Data Flow apps metadata plugin to update the port-mapping into your configuration.
The file `dataflow-configuration-port-mapping.properties` gets generated based on your inbound/outbound destination configurations.

```
      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>properties-maven-plugin</artifactId>
        <version>1.0.0</version>
        <executions>
          <execution>
            <phase>process-classes</phase>
            <goals>
              <goal>read-project-properties</goal>
            </goals>
            <configuration>
              <files>
                <file>
                  ${project.build.outputDirectory}/META-INF/dataflow-configuration-port-mapping.properties
                </file>
              </files>
            </configuration>
          </execution>
        </executions>
      </plugin>
      <plugin>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-dataflow-apps-metadata-plugin</artifactId>
        <version>1.0.3</version>
        <executions>
          <execution>
            <id>aggregate-metadata</id>
            <phase>compile</phase>
            <goals>
              <goal>aggregate-metadata</goal>
            </goals>
          </execution>
        </executions>
      </plugin>
```
<% end %>

After building the application and registering it with SCDF, you can wire this application into a stream.

<p class="note"><strong>Note:</strong> Ensure that you correctly map the logical names of the function bindings to the inbound and outbound ports. SCDF uses the logical names to wire up the applications in a stream.</p>

## <a id='multi-io-support-Q3'></a> Is there a sample application demonstrating multi-IO support?

You can use a set of ready-made sample applications to see multi-IO support in action.
Download the sample code from the [spring-cloud/spring-cloud-dataflow-samples](https://github.com/spring-cloud/spring-cloud-dataflow-samples/tree/master/multi-io-samples/kafka-streams/user-clicks-per-region) repository on GitHub.

The sample applications demonstrate how to build a streaming pipeline using a KStream processor that has multiple inputs and a single output.
The pipeline captures user click events (the number of clicks per user) and user region events (the user region) as HTTP events.
The KStream processor application includes a function that joins the `user-clicks` and `user-regions` and then computes the number of user clicks per region.
Finally, a sink application logs the output of the KStream processor.

Begin by registering the sample applications.
Visit the SCDF App Registry and bulk import the following applications:

<% if vars.scdf_product == 'K8s' %>
```
source.clicks-ingest=docker://springcloudstream/multi-io-sample-http-click-ingest:1.2.0-SNAPSHOT
source.regions-ingest=docker://springcloudstream/multi-io-sample-http-region-ingest:1.2.0-SNAPSHOT
processor.clicks-per-region=docker://springcloudstream/multi-io-sample-user-clicks-per-region-processor:1.2.0-SNAPSHOT
sink.clicks-per-region-logger=docker://springcloudstream/multi-io-sample-log-user-clicks-per-region:1.2.0-SNAPSHOT
```
<% end %>

<% if vars.scdf_product == 'TAS' %>
```
source.clicks-ingest=maven://org.springframework.cloud.dataflow.samples:http-click-ingest:1.2.0-SNAPSHOT
source.regions-ingest=maven://org.springframework.cloud.dataflow.samples:http-region-ingest:1.2.0-SNAPSHOT
processor.clicks-per-region=maven://org.springframework.cloud.dataflow.samples:user-clicks-per-region-processor:1.2.0-SNAPSHOT
sink.clicks-per-region-logger=maven://org.springframework.cloud.dataflow.samples:log-user-clicks-per-region:1.2.0-SNAPSHOT
```
<% end %>

After importing the applications, create a streaming pipeline using the Stream Editor:

![Multi-IO Streaming Pipeline](images/multi-io-stream.png)

You can also deploy a stream created using the multi-IO stream DSL:

```
clicks-ingest --server.port=9001 || regions-ingest --server.port=9000 || clicks-per-region :userRegions<:regions-ingest.user-regions :userClicks<:clicks-ingest.user-clicks || clicks-per-region-logger :_<:clicks-per-region.clicksPerRegion
```

`clicks-per-region` is a KStream application that has two inputs and one output.

<% if vars.scdf_product == 'K8s' %>
<p class="note"><strong>Note:</strong> To post HTTP events, you may need to either use port forwarding for your pods (if using Minikube) or use the <code>click-ingest</code> and <code>regions-ingest</code> deployments' Service resource endpoints (if available). This example assumes that the sample applications have been deployed on Minikube and that you are using port forwarding for the associated pods.</p>
<% end %>

Post several HTTP events as user region events:

```
curl -X POST http://localhost:9000 -H "username: Glenn" -d "americas" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Soby" -d "americas" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Janne" -d "europe" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: David" -d "americas" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Ilaya" -d "americas" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Mark" -d "americas" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Sabby" -d "americas" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Ilaya" -d "asia" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Chris" -d "americas" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Damien" -d "europe" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Christian" -d "europe" -H "Content-Type: text/plain"
curl -X POST http://localhost:9000 -H "username: Thomas" -d "americas" -H "Content-Type: text/plain"
```

You can also post HTTP events as user click events:

```
curl -X POST http://localhost:9001 -H "username: Glenn" -d 9 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: Soby" -d 15 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: Janne" -d 10 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: Mark" -d 7 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: David" -d 15 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: Sabby" -d 20 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: Ilaya" -d 10 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: Chris" -d 5 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: Damien" -d 21 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: Christian" -d 12 -H "Content-Type: text/plain"
curl -X POST http://localhost:9001 -H "username: Thomas" -d 12 -H "Content-Type: text/plain"
```

After both the user click events and the user region events have been posted, the KStream processor will begin to process the user clicks per region.
You can see the result in the logs of the logger application:

```
2020-08-04 21:53:36.735  INFO 1 --- [container-0-C-1] user-clicks-per-region                   : europe: 139
2020-08-04 21:53:36.751  INFO 1 --- [container-0-C-1] user-clicks-per-region                   : americas: 297
2020-08-04 21:53:36.754  INFO 1 --- [container-0-C-1] user-clicks-per-region                   : asia: 40
2020-08-04 21:53:37.623  INFO 1 --- [container-0-C-1] user-clicks-per-region                   : europe: 172
2020-08-04 21:53:37.625  INFO 1 --- [container-0-C-1] user-clicks-per-region                   : americas: 314

```

## <a id='multi-io-support-Q4'></a> How does multi-IO support differ from adding a tap to a stream?

When you tap the output of a streaming application in order to create a parallel pipeline, the tap is added to that output of the application.
The output acts as a publisher for all pipeline consumers that tap into it, and each of the consumers belongs to a unique consumer group.
Tapping works only for the output of a stream application.

With the new multi-IO support, SCDF binds an application that has multiple inputs and outputs.
Each input or output has its own functional binding, and SCDF maps the functional binding of the application to a destination at the messaging system.

## <a id='multi-io-support-Q5'></a> How can I replicate multi-IO support in SCDF OSS?

Multi-IO support is currently available only as a feature of the SCDF Pro server and SCDF Pro dashboard.
To deploy multi-IO streaming applications using SCDF OSS, you must use the `App` type and provide manual configuration for the functional bindings of the individual applications.
